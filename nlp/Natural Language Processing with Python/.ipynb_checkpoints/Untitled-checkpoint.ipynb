{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NER tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "\n",
    "import os\n",
    "java_path = r\"C:\\Program Files\\Java\\jdk1.8.0_131\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "nltk.data.path.append(r'C:\\Users\\dehaeth\\Documents\\Tools\\nltk')\n",
    "\n",
    "st = StanfordNERTagger(r'C:\\Users\\dehaeth\\Documents\\Tools\\nltk\\taggers\\stanford-ner-2017-06-09\\classifiers\\english.all.3class.distsim.crf.ser.gz',\n",
    "r'C:\\Users\\dehaeth\\Documents\\Tools\\nltk\\taggers\\stanford-ner-2017-06-09\\stanford-ner.jar',\n",
    "encoding='utf-8')\n",
    "\n",
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing their accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/usr/share/wikigold.conll.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-35445f701ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_annotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/usr/share/wikigold.conll.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msplit_annotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_annotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# Amend class annotations to reflect Stanford's NERTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_annotations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/share/wikigold.conll.txt'"
     ]
    }
   ],
   "source": [
    "raw_annotations = open(\"/usr/share/wikigold.conll.txt\").read()\n",
    "split_annotations = raw_annotations.split()\n",
    "\n",
    "# Amend class annotations to reflect Stanford's NERTagger\n",
    "for n,i in enumerate(split_annotations):\n",
    "    if i == \"I-PER\":\n",
    "        split_annotations[n] = \"PERSON\"\n",
    "    if i == \"I-ORG\":\n",
    "        split_annotations[n] = \"ORGANIZATION\"\n",
    "    if i == \"I-LOC\":\n",
    "        split_annotations[n] = \"LOCATION\"\n",
    "\n",
    "# Group NE data into tuples\n",
    "def group(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        val = lst[i:i+n]\n",
    "        if len(val) == n:\n",
    "            yield tuple(val)\n",
    "\n",
    "reference_annotations = list(group(split_annotations, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pure_tokens = split_annotations[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pure_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-18bb13ddf035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagged_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpure_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk_unformatted_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m#Convert prediction to multiline string and then to list (includes pos tags)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmultiline_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree2conllstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk_unformatted_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pure_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "tagged_words = nltk.pos_tag(pure_tokens)\n",
    "nltk_unformatted_prediction = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "#Convert prediction to multiline string and then to list (includes pos tags)\n",
    "multiline_string = nltk.chunk.tree2conllstr(nltk_unformatted_prediction)\n",
    "listed_pos_and_ne = multiline_string.split()\n",
    "\n",
    "# Delete pos tags and rename\n",
    "del listed_pos_and_ne[1::3]\n",
    "listed_ne = listed_pos_and_ne\n",
    "\n",
    "# Amend class annotations for consistency with reference_annotations\n",
    "for n,i in enumerate(listed_ne):\n",
    "    if i == \"B-PERSON\":\n",
    "        listed_ne[n] = \"PERSON\"\n",
    "    if i == \"I-PERSON\":\n",
    "        listed_ne[n] = \"PERSON\"    \n",
    "    if i == \"B-ORGANIZATION\":\n",
    "        listed_ne[n] = \"ORGANIZATION\"\n",
    "    if i == \"I-ORGANIZATION\":\n",
    "        listed_ne[n] = \"ORGANIZATION\"\n",
    "    if i == \"B-LOCATION\":\n",
    "        listed_ne[n] = \"LOCATION\"\n",
    "    if i == \"I-LOCATION\":\n",
    "        listed_ne[n] = \"LOCATION\"\n",
    "    if i == \"B-GPE\":\n",
    "        listed_ne[n] = \"LOCATION\"\n",
    "    if i == \"I-GPE\":\n",
    "        listed_ne[n] = \"LOCATION\"\n",
    "\n",
    "# Group prediction into tuples\n",
    "nltk_formatted_prediction = list(group(listed_ne, 2))\n",
    "\n",
    "nltk_accuracy = accuracy(reference_annotations, nltk_formatted_prediction)\n",
    "print(nltk_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dd34b062f17a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n\u001b[1;32m      2\u001b[0m \u001b[1;34m'/usr/share/stanford-ner/stanford-ner.jar'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m  encoding='utf-8')                  \n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstanford_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpure_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstanford_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreference_annotations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstanford_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         self._stanford_model = find_file(model_filename,\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    717\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[1;32m    718\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 719\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 635\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[1;31m# Check environment variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "'/usr/share/stanford-ner/stanford-ner.jar',\n",
    " encoding='utf-8')                  \n",
    "stanford_prediction = st.tag(pure_tokens)\n",
    "stanford_accuracy = accuracy(reference_annotations, stanford_prediction)\n",
    "print(stanford_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "N = 1\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "stanford_percentage = stanford_accuracy * 100\n",
    "rects1 = ax.bar(ind, stanford_percentage, width, color='r')\n",
    "\n",
    "nltk_percentage = nltk_accuracy * 100\n",
    "rects2 = ax.bar(ind+width, nltk_percentage, width, color='y')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_xlabel('Classifier')\n",
    "ax.set_ylabel('Accuracy (by percentage)')\n",
    "ax.set_title('Accuracy by NER Classifier')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( ('') )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Stanford', 'NLTK'), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2., 1.02*height, '%10.2f' % float(height),\n",
    "    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
